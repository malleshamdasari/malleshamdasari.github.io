{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed5bac05",
   "metadata": {},
   "source": [
    "# AutoEncoder on MNIST (PyTorch) — Beginner Friendly ✅\n",
    "\n",
    "This notebook trains a tiny **AutoEncoder** on the **MNIST** dataset, reconstructs digits, and prints the **compression ratio**. It’s written for beginners with lots of comments.\n",
    "\n",
    "**What you’ll see:**\n",
    "- A simple AutoEncoder model (fully-connected)\n",
    "- Training loop on MNIST\n",
    "- Reconstruction demo for one image\n",
    "- Compression ratio calculation\n",
    "- Clean visualizations\n",
    "\n",
    "> Tip: No GPU required. Runs on CPU just fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "186a4635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# 0) Setup: imports and device selection\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Use GPU if available, otherwise CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1662d7f1",
   "metadata": {},
   "source": [
    "## 1) Define a Simple AutoEncoder\n",
    "We use a fully-connected (MLP) autoencoder:\n",
    "- **Encoder** compresses a 28×28 image (784 features) → 64-D latent vector.\n",
    "- **Decoder** reconstructs 64 → 784 (back to 28×28)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4c4c746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoEncoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=784, out_features=128, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=784, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A tiny fully-connected AutoEncoder for 28x28 grayscale images.\n",
    "    - Encoder: 784 -> 128 -> 64 (latent)\n",
    "    - Decoder: 64 -> 128 -> 784 -> reshape to (1,28,28)\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim: int = 64):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28*28, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 28*28),\n",
    "            nn.Sigmoid(),  # pixel values in [0,1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        x_hat = x_hat.view(-1, 1, 28, 28)  # reshape back to image\n",
    "        return x_hat\n",
    "\n",
    "model = AutoEncoder(latent_dim=64).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b2c52d",
   "metadata": {},
   "source": [
    "## 2) Load MNIST (handwritten digits)\n",
    "We normalize to `[0,1]` using `ToTensor()`. No fancy preprocessing needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4838e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "918b63a570de4d47800d9f65b9846d9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67837d5c270845d18185d632b45bc825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5954336ddaa466e8dc70bdf56e14378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fb29ddf14ed48b1bf0823340b403503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(60000, 10000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = transforms.ToTensor()\n",
    "train_data = datasets.MNIST(root='data', train=True, transform=transform, download=True)\n",
    "test_data  = datasets.MNIST(root='data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "test_loader  = DataLoader(test_data, batch_size=128, shuffle=False)\n",
    "\n",
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1ac1fc",
   "metadata": {},
   "source": [
    "## 3) Loss and Optimizer\n",
    "- We use **Mean Squared Error (MSE)** between the input image and its reconstruction.\n",
    "- Adam optimizer with a small learning rate works well for beginners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbf31748",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c825b9b",
   "metadata": {},
   "source": [
    "## 4) Train the AutoEncoder\n",
    "Short training (5 epochs) to keep things fast on CPU. Increase if you want better reconstructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5d85885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] - Train MSE: 0.0272\n",
      "Epoch [2/5] - Train MSE: 0.0112\n",
      "Epoch [3/5] - Train MSE: 0.0088\n",
      "Epoch [4/5] - Train MSE: 0.0078\n",
      "Epoch [5/5] - Train MSE: 0.0071\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, _ in train_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, images)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch}/{epochs}] - Train MSE: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956705e3",
   "metadata": {},
   "source": [
    "## 5) Encode & Decode One Sample + Compression Ratio\n",
    "We’ll take one test image, encode it to a 64-D vector, decode it back, then show side-by-side images and print the compression ratio:\n",
    "\n",
    "**Compression ratio = (input size) / (latent size).**\n",
    "Here: input size is 784 (28×28) and latent size is 64 → **12.25×**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f73f63c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compression Ratio: 12.25x (784 / 64)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_img, _ = test_data[0]\n",
    "    sample_img = sample_img.unsqueeze(0).to(device)  # (1,1,28,28)\n",
    "    z = model.encoder(sample_img)                    # latent (1,64)\n",
    "    recon = model.decoder(z).view(-1, 1, 28, 28)     # (1,1,28,28)\n",
    "\n",
    "    # Compute compression ratio\n",
    "    input_size = 28 * 28\n",
    "    latent_size = model.latent_dim\n",
    "    compression_ratio = input_size / latent_size\n",
    "    print(f\"Compression Ratio: {compression_ratio:.2f}x (784 / {latent_size})\")\n",
    "\n",
    "    # Move to CPU for plotting\n",
    "    orig = sample_img.squeeze().detach().cpu().numpy()\n",
    "    rec  = recon.squeeze().detach().cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(orig, cmap='gray')\n",
    "plt.title('Original')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(rec, cmap='gray')\n",
    "plt.title('Reconstructed')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e092c3",
   "metadata": {},
   "source": [
    "## 6) (Optional) Save & Load the Trained Model\n",
    "You can save the trained weights and reload later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1776bc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'autoencoder_mnist.pt'\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print('Saved to', save_path)\n",
    "\n",
    "# To load later:\n",
    "loaded = AutoEncoder(latent_dim=64).to(device)\n",
    "loaded.load_state_dict(torch.load(save_path, map_location=device))\n",
    "loaded.eval()\n",
    "print('Reloaded model ready!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0e64c8",
   "metadata": {},
   "source": [
    "## Notes & Next Steps\n",
    "- Try changing `latent_dim` (e.g., 32 or 128) to see compression-quality tradeoffs.\n",
    "- Increase `epochs` for better reconstructions.\n",
    "- Replace the MLP with a **convolutional autoencoder** for sharper images."
   ]
  }
 ],
 "metadata": {
  "created": "2025-10-08T20:00:46.230569Z",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
